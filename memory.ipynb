{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFps9x/kObIv6+SfRFYpWe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfarooq33/Task-Offloading-and-Fog-Computing/blob/master/memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ykiMHW1OShjG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This Memory File contains memory operation including encoding and decoding operations.**\n",
        "version 1.0 -- January 2018. Written by Liang Huang (lianghuang AT zjut.edu.cn)\n"
      ],
      "metadata": {
        "id": "lpeCAT7KTAZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ou8xRVrVS9Eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "taUp-kQTSVWU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VljS8oG6SJkK",
        "outputId": "4bb4f086-69f6-4516-9ae2-2eb56af9c0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:13: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:130: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:132: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:162: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:13: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:130: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:132: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:162: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<ipython-input-2-6533ba2b3825>:13: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  assert(len(net) is 4) # only 4-layer DNN\n",
            "<ipython-input-2-6533ba2b3825>:130: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if mode is 'OP':\n",
            "<ipython-input-2-6533ba2b3825>:132: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  elif mode is 'KNN':\n",
            "<ipython-input-2-6533ba2b3825>:162: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if len(self.enumerate_actions) is 0:\n"
          ]
        }
      ],
      "source": [
        "# DNN network for memory\n",
        "class MemoryDNN:\n",
        "    def __init__(\n",
        "        self,\n",
        "        net,\n",
        "        learning_rate = 0.01,\n",
        "        training_interval=10, \n",
        "        batch_size=100, \n",
        "        memory_size=1000,\n",
        "        output_graph=False\n",
        "    ):\n",
        "        # net: [n_input, n_hidden_1st, n_hidded_2ed, n_output]\n",
        "        assert(len(net) is 4) # only 4-layer DNN\n",
        "\n",
        "        self.net = net\n",
        "        self.training_interval = training_interval # learn every #training_interval\n",
        "        self.lr = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.memory_size = memory_size\n",
        "        \n",
        "        # store all binary actions\n",
        "        self.enumerate_actions = []\n",
        "\n",
        "        # stored # memory entry\n",
        "        self.memory_counter = 1\n",
        "\n",
        "        # store training cost\n",
        "        self.cost_his = []\n",
        "\n",
        "        # reset graph \n",
        "        tf.reset_default_graph()\n",
        "\n",
        "        # initialize zero memory [h, m]\n",
        "        self.memory = np.zeros((self.memory_size, self.net[0]+ self.net[-1]))\n",
        "\n",
        "        # construct memory network\n",
        "        self._build_net()\n",
        "\n",
        "        self.sess = tf.Session()\n",
        "\n",
        "        # for tensorboard\n",
        "        if output_graph:\n",
        "            # $ tensorboard --logdir=logs\n",
        "            # tf.train.SummaryWriter soon be deprecated, use following\n",
        "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
        "\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "    def _build_net(self):\n",
        "        def build_layers(h, c_names, net, w_initializer, b_initializer):\n",
        "            with tf.variable_scope('l1'):\n",
        "                w1 = tf.get_variable('w1', [net[0], net[1]], initializer=w_initializer, collections=c_names)\n",
        "                b1 = tf.get_variable('b1', [1, self.net[1]], initializer=b_initializer, collections=c_names)\n",
        "                l1 = tf.nn.relu(tf.matmul(h, w1) + b1)\n",
        "\n",
        "            with tf.variable_scope('l2'):\n",
        "                w2 = tf.get_variable('w2', [net[1], net[2]], initializer=w_initializer, collections=c_names)\n",
        "                b2 = tf.get_variable('b2', [1, net[2]], initializer=b_initializer, collections=c_names)\n",
        "                l2 = tf.nn.relu(tf.matmul(l1, w2) + b2)\n",
        "\n",
        "            with tf.variable_scope('M'):\n",
        "                w3 = tf.get_variable('w3', [net[2], net[3]], initializer=w_initializer, collections=c_names)\n",
        "                b3 = tf.get_variable('b3', [1, net[3]], initializer=b_initializer, collections=c_names)\n",
        "                out = tf.matmul(l2, w3) + b3\n",
        "\n",
        "            return out\n",
        "\n",
        "        # ------------------ build memory_net ------------------\n",
        "        self.h = tf.placeholder(tf.float32, [None, self.net[0]], name='h')  # input\n",
        "        self.m = tf.placeholder(tf.float32, [None, self.net[-1]], name='mode')  # for calculating loss\n",
        "        self.is_train = tf.placeholder(\"bool\") # train or evaluate\n",
        "\n",
        "        with tf.variable_scope('memory_net'):\n",
        "            c_names, w_initializer, b_initializer = \\\n",
        "                ['memory_net_params', tf.GraphKeys.GLOBAL_VARIABLES], \\\n",
        "                tf.random_normal_initializer(0., 1/self.net[0]), tf.constant_initializer(0.1)  # config of layers\n",
        "\n",
        "            self.m_pred = build_layers(self.h, c_names, self.net, w_initializer, b_initializer)\n",
        "\n",
        "        with tf.variable_scope('loss'):\n",
        "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = self.m, logits = self.m_pred))\n",
        "\n",
        "        with tf.variable_scope('train'):\n",
        "            self._train_op = tf.train.AdamOptimizer(self.lr, 0.09).minimize(self.loss)\n",
        "\n",
        "\n",
        "    def remember(self, h, m):\n",
        "        # replace the old memory with new memory\n",
        "        idx = self.memory_counter % self.memory_size\n",
        "        self.memory[idx, :] = np.hstack((h,m))\n",
        "\n",
        "        self.memory_counter += 1\n",
        "\n",
        "    def encode(self, h, m):\n",
        "        # encoding the entry\n",
        "        self.remember(h, m)\n",
        "        # train the DNN every 10 step\n",
        "#        if self.memory_counter> self.memory_size / 2 and self.memory_counter % self.training_interval == 0:\n",
        "        if self.memory_counter % self.training_interval == 0:\n",
        "            self.learn()\n",
        "\n",
        "    def learn(self):\n",
        "        # sample batch memory from all memory\n",
        "        if self.memory_counter > self.memory_size:\n",
        "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
        "        else:\n",
        "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
        "        batch_memory = self.memory[sample_index, :]\n",
        "        \n",
        "        h_train = batch_memory[:, 0: self.net[0]]\n",
        "        m_train = batch_memory[:, self.net[0]:]\n",
        "        \n",
        "        # print(h_train)\n",
        "        # print(m_train)\n",
        "\n",
        "        # train the DNN\n",
        "        _, self.cost = self.sess.run([self._train_op, self.loss], \n",
        "                                         feed_dict={self.h: h_train, self.m: m_train})\n",
        "\n",
        "        assert(self.cost >0)    \n",
        "        self.cost_his.append(self.cost)\n",
        "\n",
        "    def decode(self, h, k = 1, mode = 'OP'):\n",
        "        # to have batch dimension when feed into tf placeholder\n",
        "        h = h[np.newaxis, :]\n",
        "\n",
        "        m_pred = self.sess.run(self.m_pred, feed_dict={self.h: h})\n",
        "\n",
        "        if mode is 'OP':\n",
        "            return self.knm(m_pred[0], k)\n",
        "        elif mode is 'KNN':\n",
        "            return self.knn(m_pred[0], k)\n",
        "        else:\n",
        "            print(\"The action selection must be 'OP' or 'KNN'\")\n",
        "    \n",
        "    def knm(self, m, k = 1):\n",
        "        # return k-nearest-mode\n",
        "        m_list = []\n",
        "        \n",
        "        # generate the ﬁrst binary ofﬂoading decision \n",
        "        # note that here 'm' is the output of DNN before the sigmoid activation function, in the field of all real number. \n",
        "        # Therefore, we compare it with '0' instead of 0.5 in equation (8). Since, sigmod(0) = 0.5.\n",
        "        m_list.append(1*(m>0))\n",
        "        \n",
        "        if k > 1:\n",
        "            # generate the remaining K-1 binary ofﬂoading decisions with respect to equation (9)\n",
        "            m_abs = abs(m)\n",
        "            idx_list = np.argsort(m_abs)[:k-1]\n",
        "            for i in range(k-1):\n",
        "                if m[idx_list[i]] >0:\n",
        "                    # set a positive user to 0\n",
        "                    m_list.append(1*(m - m[idx_list[i]] > 0))\n",
        "                else:\n",
        "                    # set a negtive user to 1\n",
        "                    m_list.append(1*(m - m[idx_list[i]] >= 0))\n",
        "\n",
        "        return m_list\n",
        "    \n",
        "    def knn(self, m, k = 1):\n",
        "        # list all 2^N binary offloading actions\n",
        "        if len(self.enumerate_actions) is 0:\n",
        "            import itertools\n",
        "            self.enumerate_actions = np.array(list(map(list, itertools.product([0, 1], repeat=self.net[0]))))\n",
        "\n",
        "        # the 2-norm\n",
        "        sqd = ((self.enumerate_actions - m)**2).sum(1)\n",
        "        idx = np.argsort(sqd)\n",
        "        return self.enumerate_actions[idx[:k]]\n",
        "        \n",
        "\n",
        "    def plot_cost(self):\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.plot(np.arange(len(self.cost_his))*self.training_interval, self.cost_his)\n",
        "        plt.ylabel('Training Loss')\n",
        "        plt.xlabel('Time Frames')\n",
        "        plt.show()"
      ]
    }
  ]
}