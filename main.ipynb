{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGw6GfH7V4bE9B/kfbRbog",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfarooq33/Task-Offloading-and-Fog-Computing/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcZqNc_bGfkz"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp content/drive/MyDrive/Importing\\ Scripts \\as\\ Modules/DROO-master/main.py /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_yN8BR3G7bN",
        "outputId": "bc83f6e3-f21c-4fcc-b7b3-6e92d2b68de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'content/drive/MyDrive/Importing Scripts': No such file or directory\n",
            "cp: cannot stat 'as Modules/DROO-master/main.py': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  #################################################################\n",
        "#  Deep Reinforcement Learning for Online Ofﬂoading in Wireless Powered Mobile-Edge Computing Networks\n",
        "#\n",
        "#  This file contains the main code of DROO. It loads the training samples saved in ./data/data_#.mat, splits the samples into two parts (training and testing data constitutes 80% and 20%), trains the DNN with training and validation samples, and finally tests the DNN with test data.\n",
        "#\n",
        "#  Input: ./data/data_#.mat\n",
        "#    Data samples are generated according to the CD method presented in [2]. There are 30,000 samples saved in each ./data/data_#.mat, where # is the user number. Each data sample includes\n",
        "#  -----------------------------------------------------------------\n",
        "#  |       wireless channel gain           |    input_h            |\n",
        "#  -----------------------------------------------------------------\n",
        "#  |       computing mode selection        |    output_mode        |\n",
        "#  -----------------------------------------------------------------\n",
        "#  |       energy broadcasting parameter   |    output_a           |\n",
        "#  -----------------------------------------------------------------\n",
        "#  |     transmit time of wireless device  |    output_tau         |\n",
        "#  -----------------------------------------------------------------\n",
        "#  |      weighted sum computation rate    |    output_obj         |\n",
        "#  -----------------------------------------------------------------\n",
        "#\n",
        "#\n",
        "#  References:\n",
        "#  [1] 1. Liang Huang, Suzhi Bi, and Ying-Jun Angela Zhang, \"Deep Reinforcement Learning for Online Offloading in Wireless Powered Mobile-Edge Computing Networks,\" in IEEE Transactions on Mobile Computing, early access, 2019, DOI:10.1109/TMC.2019.2928811.\n",
        "#  [2] S. Bi and Y. J. Zhang, “Computation rate maximization for wireless powered mobile-edge computing with binary computation ofﬂoading,” IEEE Trans. Wireless Commun., vol. 17, no. 6, pp. 4177-4190, Jun. 2018.\n",
        "#\n",
        "# version 1.0 -- July 2018. Written by Liang Huang (lianghuang AT zjut.edu.cn)\n",
        "#  #################################################################\n",
        "\n",
        "\n",
        "import scipy.io as sio                     # import scipy.io for .mat file I/\n",
        "import numpy as np                         # import numpy\n",
        "\n",
        "from memory import MemoryDNN\n",
        "from optimization import bisection\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "def plot_rate( rate_his, rolling_intv = 50):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    import matplotlib as mpl\n",
        "\n",
        "    rate_array = np.asarray(rate_his)\n",
        "    df = pd.DataFrame(rate_his)\n",
        "\n",
        "\n",
        "    mpl.style.use('seaborn')\n",
        "    fig, ax = plt.subplots(figsize=(15,8))\n",
        "#    rolling_intv = 20\n",
        "\n",
        "    plt.plot(np.arange(len(rate_array))+1, np.hstack(df.rolling(rolling_intv, min_periods=1).mean().values), 'b')\n",
        "    plt.fill_between(np.arange(len(rate_array))+1, np.hstack(df.rolling(rolling_intv, min_periods=1).min()[0].values), np.hstack(df.rolling(rolling_intv, min_periods=1).max()[0].values), color = 'b', alpha = 0.2)\n",
        "    plt.ylabel('Normalized Computation Rate')\n",
        "    plt.xlabel('Time Frames')\n",
        "    plt.show()\n",
        "\n",
        "def save_to_txt(rate_his, file_path):\n",
        "    with open(file_path, 'w') as f:\n",
        "        for rate in rate_his:\n",
        "            f.write(\"%s \\n\" % rate)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    '''\n",
        "        This algorithm generates K modes from DNN, and chooses with largest\n",
        "        reward. The mode with largest reward is stored in the memory, which is\n",
        "        further used to train the DNN.\n",
        "        Adaptive K is implemented. K = max(K, K_his[-memory_size])\n",
        "    '''\n",
        "\n",
        "    N = 10                     # number of users\n",
        "    n = 30000                     # number of time frames\n",
        "    K = N                   # initialize K = N\n",
        "    decoder_mode = 'OP'    # the quantization mode could be 'OP' (Order-preserving) or 'KNN'\n",
        "    Memory = 1024          # capacity of memory structure\n",
        "    Delta = 32             # Update interval for adaptive K\n",
        "\n",
        "    print('#user = %d, #channel=%d, K=%d, decoder = %s, Memory = %d, Delta = %d'%(N,n,K,decoder_mode, Memory, Delta))\n",
        "    # Load data\n",
        "    channel = sio.loadmat('./data/data_%d' %N)['input_h']\n",
        "    rate = sio.loadmat('./data/data_%d' %N)['output_obj'] # this rate is only used to plot figures; never used to train DROO.\n",
        "\n",
        "    # increase h to close to 1 for better training; it is a trick widely adopted in deep learning\n",
        "    channel = channel * 1000000\n",
        "\n",
        "    # generate the train and test data sample index\n",
        "    # data are splitted as 80:20\n",
        "    # training data are randomly sampled with duplication if n > total data size\n",
        "\n",
        "    split_idx = int(.8* len(channel))\n",
        "    num_test = min(len(channel) - split_idx, n - int(.8* n)) # training data size\n",
        "\n",
        "\n",
        "    mem = MemoryDNN(net = [N, 120, 80, N],\n",
        "                    learning_rate = 0.01,\n",
        "                    training_interval=10,\n",
        "                    batch_size=128,\n",
        "                    memory_size=Memory\n",
        "                    )\n",
        "\n",
        "    start_time=time.time()\n",
        "\n",
        "    rate_his = []\n",
        "    rate_his_ratio = []\n",
        "    mode_his = []\n",
        "    k_idx_his = []\n",
        "    K_his = []\n",
        "    for i in range(n):\n",
        "        if i % (n//10) == 0:\n",
        "           print(\"%0.1f\"%(i/n))\n",
        "        if i> 0 and i % Delta == 0:\n",
        "            # index counts from 0\n",
        "            if Delta > 1:\n",
        "                max_k = max(k_idx_his[-Delta:-1]) +1;\n",
        "            else:\n",
        "                max_k = k_idx_his[-1] +1;\n",
        "            K = min(max_k +1, N)\n",
        "\n",
        "        if i < n - num_test:\n",
        "            # training\n",
        "            i_idx = i % split_idx\n",
        "        else:\n",
        "            # test\n",
        "            i_idx = i - n + num_test + split_idx\n",
        "\n",
        "        h = channel[i_idx,:]\n",
        "\n",
        "        # the action selection must be either 'OP' or 'KNN'\n",
        "        m_list = mem.decode(h, K, decoder_mode)\n",
        "\n",
        "        r_list = []\n",
        "        for m in m_list:\n",
        "            r_list.append(bisection(h/1000000, m)[0])\n",
        "            \n",
        "        # encode the mode with largest reward\n",
        "        mem.encode(h, m_list[np.argmax(r_list)])\n",
        "        # the main code for DROO training ends here\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        # the following codes store some interested metrics for illustrations\n",
        "        # memorize the largest reward\n",
        "        rate_his.append(np.max(r_list))\n",
        "        rate_his_ratio.append(rate_his[-1] / rate[i_idx][0])\n",
        "        # record the index of largest reward\n",
        "        k_idx_his.append(np.argmax(r_list))\n",
        "        # record K in case of adaptive K\n",
        "        K_his.append(K)\n",
        "        mode_his.append(m_list[np.argmax(r_list)])\n",
        "\n",
        "\n",
        "    total_time=time.time()-start_time\n",
        "    mem.plot_cost()\n",
        "    plot_rate(rate_his_ratio)\n",
        "\n",
        "    print(\"Averaged normalized computation rate:\", sum(rate_his_ratio[-num_test: -1])/num_test)\n",
        "    print('Total time consumed:%s'%total_time)\n",
        "    print('Average time per channel:%s'%(total_time/n))\n",
        "\n",
        "    # save data into txt\n",
        "    save_to_txt(k_idx_his, \"k_idx_his.txt\")\n",
        "    save_to_txt(K_his, \"K_his.txt\")\n",
        "    save_to_txt(mem.cost_his, \"cost_his.txt\")\n",
        "    save_to_txt(rate_his_ratio, \"rate_his_ratio.txt\")\n",
        "    save_to_txt(mode_his, \"mode_his.txt\")"
      ],
      "metadata": {
        "id": "eyzq8nm0PnRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "MkqI9XbFHDmT"
      }
    }
  ]
}